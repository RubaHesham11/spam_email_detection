{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMk2aq2SD4TjbbnlZGvKEdM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubaHesham11/spam_email_detection/blob/main/spam_email_detection_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weu3vjJX_bo0",
        "outputId": "d2f28a37-4e00-43f3-b68c-a3710169a53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.3-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "rnv-yzjBBMc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = spambase.data.features\n",
        "y = spambase.data.targets\n",
        "\n",
        "# metadata\n",
        "print(spambase.metadata)\n",
        "\n",
        "# variable information\n",
        "print(spambase.variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhFYkppOAXHb",
        "outputId": "b8ce33fe-1781-4e90-e965-59c08a64f027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 94, 'name': 'Spambase', 'repository_url': 'https://archive.ics.uci.edu/dataset/94/spambase', 'data_url': 'https://archive.ics.uci.edu/static/public/94/data.csv', 'abstract': 'Classifying Email as Spam or Non-Spam', 'area': 'Computer Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 4601, 'num_features': 57, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1999, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C53G6X', 'creators': ['Mark Hopkins', 'Erik Reeber', 'George Forman', 'Jaap Suermondt'], 'intro_paper': None, 'additional_info': {'summary': 'The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography...\\n\\nThe classification task for this dataset is to determine whether a given email is spam or not.\\n\\t\\nOur collection of spam e-mails came from our postmaster and individuals who had filed spam.  Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word \\'george\\' and the area code \\'650\\' are indicators of non-spam.  These are useful when constructing a personalized spam filter.  One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\\n\\nFor background on spam: Cranor, Lorrie F., LaMacchia, Brian A.  Spam!, Communications of the ACM, 41(8):74-83, 1998.\\n\\nTypical performance is around ~7% misclassification error. False positives (marking good mail as spam) are very undesirable.If we insist on zero false positives in the training/testing set, 20-25% of the spam passed through the filter. See also Hewlett-Packard Internal-only Technical Report. External version forthcoming. ', 'purpose': None, 'funded_by': None, 'instances_represent': 'Emails', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The last column of \\'spambase.data\\' denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail.  The run-length attributes (55-57) measure the length of sequences of consecutive capital letters.  For the statistical measures of each attribute, see the end of this file.  Here are the definitions of the attributes:\\r\\n\\r\\n48 continuous real [0,100] attributes of type word_freq_WORD \\r\\n= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail.  A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\\r\\n\\r\\n6 continuous real [0,100] attributes of type char_freq_CHAR] \\r\\n= percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail\\r\\n\\r\\n1 continuous real [1,...] attribute of type capital_run_length_average \\r\\n= average length of uninterrupted sequences of capital letters\\r\\n\\r\\n1 continuous integer [1,...] attribute of type capital_run_length_longest \\r\\n= length of longest uninterrupted sequence of capital letters\\r\\n\\r\\n1 continuous integer [1,...] attribute of type capital_run_length_total \\r\\n= sum of length of uninterrupted sequences of capital letters \\r\\n= total number of capital letters in the e-mail\\r\\n\\r\\n1 nominal {0,1} class attribute of type spam\\r\\n= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \\r\\n', 'citation': None}}\n",
            "                          name     role        type demographic  \\\n",
            "0               word_freq_make  Feature  Continuous        None   \n",
            "1            word_freq_address  Feature  Continuous        None   \n",
            "2                word_freq_all  Feature  Continuous        None   \n",
            "3                 word_freq_3d  Feature  Continuous        None   \n",
            "4                word_freq_our  Feature  Continuous        None   \n",
            "5               word_freq_over  Feature  Continuous        None   \n",
            "6             word_freq_remove  Feature  Continuous        None   \n",
            "7           word_freq_internet  Feature  Continuous        None   \n",
            "8              word_freq_order  Feature  Continuous        None   \n",
            "9               word_freq_mail  Feature  Continuous        None   \n",
            "10           word_freq_receive  Feature  Continuous        None   \n",
            "11              word_freq_will  Feature  Continuous        None   \n",
            "12            word_freq_people  Feature  Continuous        None   \n",
            "13            word_freq_report  Feature  Continuous        None   \n",
            "14         word_freq_addresses  Feature  Continuous        None   \n",
            "15              word_freq_free  Feature  Continuous        None   \n",
            "16          word_freq_business  Feature  Continuous        None   \n",
            "17             word_freq_email  Feature  Continuous        None   \n",
            "18               word_freq_you  Feature  Continuous        None   \n",
            "19            word_freq_credit  Feature  Continuous        None   \n",
            "20              word_freq_your  Feature  Continuous        None   \n",
            "21              word_freq_font  Feature  Continuous        None   \n",
            "22               word_freq_000  Feature  Continuous        None   \n",
            "23             word_freq_money  Feature  Continuous        None   \n",
            "24                word_freq_hp  Feature  Continuous        None   \n",
            "25               word_freq_hpl  Feature  Continuous        None   \n",
            "26            word_freq_george  Feature  Continuous        None   \n",
            "27               word_freq_650  Feature  Continuous        None   \n",
            "28               word_freq_lab  Feature  Continuous        None   \n",
            "29              word_freq_labs  Feature  Continuous        None   \n",
            "30            word_freq_telnet  Feature  Continuous        None   \n",
            "31               word_freq_857  Feature  Continuous        None   \n",
            "32              word_freq_data  Feature  Continuous        None   \n",
            "33               word_freq_415  Feature  Continuous        None   \n",
            "34                word_freq_85  Feature  Continuous        None   \n",
            "35        word_freq_technology  Feature  Continuous        None   \n",
            "36              word_freq_1999  Feature  Continuous        None   \n",
            "37             word_freq_parts  Feature  Continuous        None   \n",
            "38                word_freq_pm  Feature  Continuous        None   \n",
            "39            word_freq_direct  Feature  Continuous        None   \n",
            "40                word_freq_cs  Feature  Continuous        None   \n",
            "41           word_freq_meeting  Feature  Continuous        None   \n",
            "42          word_freq_original  Feature  Continuous        None   \n",
            "43           word_freq_project  Feature  Continuous        None   \n",
            "44                word_freq_re  Feature  Continuous        None   \n",
            "45               word_freq_edu  Feature  Continuous        None   \n",
            "46             word_freq_table  Feature  Continuous        None   \n",
            "47        word_freq_conference  Feature  Continuous        None   \n",
            "48                 char_freq_;  Feature  Continuous        None   \n",
            "49                 char_freq_(  Feature  Continuous        None   \n",
            "50                 char_freq_[  Feature  Continuous        None   \n",
            "51                 char_freq_!  Feature  Continuous        None   \n",
            "52                 char_freq_$  Feature  Continuous        None   \n",
            "53                 char_freq_#  Feature  Continuous        None   \n",
            "54  capital_run_length_average  Feature  Continuous        None   \n",
            "55  capital_run_length_longest  Feature  Continuous        None   \n",
            "56    capital_run_length_total  Feature  Continuous        None   \n",
            "57                       Class   Target      Binary        None   \n",
            "\n",
            "                 description units missing_values  \n",
            "0                       None  None             no  \n",
            "1                       None  None             no  \n",
            "2                       None  None             no  \n",
            "3                       None  None             no  \n",
            "4                       None  None             no  \n",
            "5                       None  None             no  \n",
            "6                       None  None             no  \n",
            "7                       None  None             no  \n",
            "8                       None  None             no  \n",
            "9                       None  None             no  \n",
            "10                      None  None             no  \n",
            "11                      None  None             no  \n",
            "12                      None  None             no  \n",
            "13                      None  None             no  \n",
            "14                      None  None             no  \n",
            "15                      None  None             no  \n",
            "16                      None  None             no  \n",
            "17                      None  None             no  \n",
            "18                      None  None             no  \n",
            "19                      None  None             no  \n",
            "20                      None  None             no  \n",
            "21                      None  None             no  \n",
            "22                      None  None             no  \n",
            "23                      None  None             no  \n",
            "24                      None  None             no  \n",
            "25                      None  None             no  \n",
            "26                      None  None             no  \n",
            "27                      None  None             no  \n",
            "28                      None  None             no  \n",
            "29                      None  None             no  \n",
            "30                      None  None             no  \n",
            "31                      None  None             no  \n",
            "32                      None  None             no  \n",
            "33                      None  None             no  \n",
            "34                      None  None             no  \n",
            "35                      None  None             no  \n",
            "36                      None  None             no  \n",
            "37                      None  None             no  \n",
            "38                      None  None             no  \n",
            "39                      None  None             no  \n",
            "40                      None  None             no  \n",
            "41                      None  None             no  \n",
            "42                      None  None             no  \n",
            "43                      None  None             no  \n",
            "44                      None  None             no  \n",
            "45                      None  None             no  \n",
            "46                      None  None             no  \n",
            "47                      None  None             no  \n",
            "48                      None  None             no  \n",
            "49                      None  None             no  \n",
            "50                      None  None             no  \n",
            "51                      None  None             no  \n",
            "52                      None  None             no  \n",
            "53                      None  None             no  \n",
            "54                      None  None             no  \n",
            "55                      None  None             no  \n",
            "56                      None  None             no  \n",
            "57  spam (1) or not spam (0)  None             no  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "metadata": {
        "id": "R6rg8yooAYWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "dTHdJhZWAYLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],))\n",
        "])"
      ],
      "metadata": {
        "id": "uOsCwMIoAXue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7M4pw4GoAW6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaMxJAQNAju3",
        "outputId": "ecab2a5f-7f80-49cb-b2aa-36910d6cb502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "92/92 [==============================] - 2s 5ms/step - loss: 0.8613 - accuracy: 0.5326 - val_loss: 0.7272 - val_accuracy: 0.6318\n",
            "Epoch 2/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.6542 - accuracy: 0.6895 - val_loss: 0.5750 - val_accuracy: 0.7405\n",
            "Epoch 3/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.5415 - accuracy: 0.7666 - val_loss: 0.4856 - val_accuracy: 0.8125\n",
            "Epoch 4/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4704 - accuracy: 0.8094 - val_loss: 0.4268 - val_accuracy: 0.8478\n",
            "Epoch 5/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4225 - accuracy: 0.8380 - val_loss: 0.3855 - val_accuracy: 0.8682\n",
            "Epoch 6/200\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.3884 - accuracy: 0.8556 - val_loss: 0.3559 - val_accuracy: 0.8886\n",
            "Epoch 7/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3630 - accuracy: 0.8709 - val_loss: 0.3338 - val_accuracy: 0.8967\n",
            "Epoch 8/200\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.3438 - accuracy: 0.8828 - val_loss: 0.3159 - val_accuracy: 0.9049\n",
            "Epoch 9/200\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8896 - val_loss: 0.3024 - val_accuracy: 0.9035\n",
            "Epoch 10/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.3158 - accuracy: 0.8930 - val_loss: 0.2914 - val_accuracy: 0.9076\n",
            "Epoch 11/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.3056 - accuracy: 0.8974 - val_loss: 0.2821 - val_accuracy: 0.9171\n",
            "Epoch 12/200\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2970 - accuracy: 0.8998 - val_loss: 0.2743 - val_accuracy: 0.9158\n",
            "Epoch 13/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2897 - accuracy: 0.9039 - val_loss: 0.2678 - val_accuracy: 0.9185\n",
            "Epoch 14/200\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2835 - accuracy: 0.9049 - val_loss: 0.2621 - val_accuracy: 0.9198\n",
            "Epoch 15/200\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2778 - accuracy: 0.9059 - val_loss: 0.2572 - val_accuracy: 0.9226\n",
            "Epoch 16/200\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2728 - accuracy: 0.9073 - val_loss: 0.2531 - val_accuracy: 0.9239\n",
            "Epoch 17/200\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.2686 - accuracy: 0.9100 - val_loss: 0.2494 - val_accuracy: 0.9266\n",
            "Epoch 18/200\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.2648 - accuracy: 0.9100 - val_loss: 0.2462 - val_accuracy: 0.9280\n",
            "Epoch 19/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2612 - accuracy: 0.9127 - val_loss: 0.2431 - val_accuracy: 0.9307\n",
            "Epoch 20/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2580 - accuracy: 0.9134 - val_loss: 0.2406 - val_accuracy: 0.9293\n",
            "Epoch 21/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2552 - accuracy: 0.9134 - val_loss: 0.2383 - val_accuracy: 0.9307\n",
            "Epoch 22/200\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2526 - accuracy: 0.9147 - val_loss: 0.2362 - val_accuracy: 0.9321\n",
            "Epoch 23/200\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.2502 - accuracy: 0.9151 - val_loss: 0.2343 - val_accuracy: 0.9307\n",
            "Epoch 24/200\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.2480 - accuracy: 0.9151 - val_loss: 0.2326 - val_accuracy: 0.9307\n",
            "Epoch 25/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2460 - accuracy: 0.9171 - val_loss: 0.2311 - val_accuracy: 0.9307\n",
            "Epoch 26/200\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2440 - accuracy: 0.9171 - val_loss: 0.2298 - val_accuracy: 0.9307\n",
            "Epoch 27/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2422 - accuracy: 0.9192 - val_loss: 0.2286 - val_accuracy: 0.9293\n",
            "Epoch 28/200\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2407 - accuracy: 0.9185 - val_loss: 0.2275 - val_accuracy: 0.9293\n",
            "Epoch 29/200\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2392 - accuracy: 0.9192 - val_loss: 0.2265 - val_accuracy: 0.9321\n",
            "Epoch 30/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2377 - accuracy: 0.9188 - val_loss: 0.2256 - val_accuracy: 0.9334\n",
            "Epoch 31/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2365 - accuracy: 0.9198 - val_loss: 0.2248 - val_accuracy: 0.9334\n",
            "Epoch 32/200\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2352 - accuracy: 0.9195 - val_loss: 0.2241 - val_accuracy: 0.9321\n",
            "Epoch 33/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2340 - accuracy: 0.9202 - val_loss: 0.2234 - val_accuracy: 0.9348\n",
            "Epoch 34/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2328 - accuracy: 0.9215 - val_loss: 0.2228 - val_accuracy: 0.9348\n",
            "Epoch 35/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2318 - accuracy: 0.9226 - val_loss: 0.2223 - val_accuracy: 0.9348\n",
            "Epoch 36/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.9219 - val_loss: 0.2217 - val_accuracy: 0.9334\n",
            "Epoch 37/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2299 - accuracy: 0.9215 - val_loss: 0.2214 - val_accuracy: 0.9334\n",
            "Epoch 38/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.9222 - val_loss: 0.2207 - val_accuracy: 0.9334\n",
            "Epoch 39/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2281 - accuracy: 0.9226 - val_loss: 0.2206 - val_accuracy: 0.9334\n",
            "Epoch 40/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2274 - accuracy: 0.9222 - val_loss: 0.2202 - val_accuracy: 0.9334\n",
            "Epoch 41/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2266 - accuracy: 0.9229 - val_loss: 0.2200 - val_accuracy: 0.9334\n",
            "Epoch 42/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2259 - accuracy: 0.9226 - val_loss: 0.2198 - val_accuracy: 0.9334\n",
            "Epoch 43/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2252 - accuracy: 0.9226 - val_loss: 0.2196 - val_accuracy: 0.9334\n",
            "Epoch 44/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2246 - accuracy: 0.9222 - val_loss: 0.2194 - val_accuracy: 0.9334\n",
            "Epoch 45/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.9236 - val_loss: 0.2194 - val_accuracy: 0.9334\n",
            "Epoch 46/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.9232 - val_loss: 0.2193 - val_accuracy: 0.9348\n",
            "Epoch 47/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2228 - accuracy: 0.9249 - val_loss: 0.2191 - val_accuracy: 0.9348\n",
            "Epoch 48/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.9243 - val_loss: 0.2190 - val_accuracy: 0.9348\n",
            "Epoch 49/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9246 - val_loss: 0.2188 - val_accuracy: 0.9348\n",
            "Epoch 50/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.9246 - val_loss: 0.2188 - val_accuracy: 0.9348\n",
            "Epoch 51/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.9249 - val_loss: 0.2187 - val_accuracy: 0.9348\n",
            "Epoch 52/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2204 - accuracy: 0.9246 - val_loss: 0.2188 - val_accuracy: 0.9348\n",
            "Epoch 53/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2199 - accuracy: 0.9246 - val_loss: 0.2187 - val_accuracy: 0.9348\n",
            "Epoch 54/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9243 - val_loss: 0.2189 - val_accuracy: 0.9348\n",
            "Epoch 55/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9246 - val_loss: 0.2189 - val_accuracy: 0.9348\n",
            "Epoch 56/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.9239 - val_loss: 0.2188 - val_accuracy: 0.9348\n",
            "Epoch 57/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9246 - val_loss: 0.2188 - val_accuracy: 0.9361\n",
            "Epoch 58/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2179 - accuracy: 0.9239 - val_loss: 0.2189 - val_accuracy: 0.9361\n",
            "Epoch 59/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.9249 - val_loss: 0.2187 - val_accuracy: 0.9375\n",
            "Epoch 60/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9243 - val_loss: 0.2190 - val_accuracy: 0.9375\n",
            "Epoch 61/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2169 - accuracy: 0.9249 - val_loss: 0.2191 - val_accuracy: 0.9375\n",
            "Epoch 62/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9249 - val_loss: 0.2190 - val_accuracy: 0.9375\n",
            "Epoch 63/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9253 - val_loss: 0.2193 - val_accuracy: 0.9389\n",
            "Epoch 64/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2160 - accuracy: 0.9256 - val_loss: 0.2193 - val_accuracy: 0.9375\n",
            "Epoch 65/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9253 - val_loss: 0.2194 - val_accuracy: 0.9375\n",
            "Epoch 66/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9249 - val_loss: 0.2194 - val_accuracy: 0.9375\n",
            "Epoch 67/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.9256 - val_loss: 0.2196 - val_accuracy: 0.9375\n",
            "Epoch 68/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9256 - val_loss: 0.2196 - val_accuracy: 0.9375\n",
            "Epoch 69/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9260 - val_loss: 0.2197 - val_accuracy: 0.9375\n",
            "Epoch 70/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2144 - accuracy: 0.9249 - val_loss: 0.2198 - val_accuracy: 0.9375\n",
            "Epoch 71/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.9249 - val_loss: 0.2199 - val_accuracy: 0.9389\n",
            "Epoch 72/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9249 - val_loss: 0.2201 - val_accuracy: 0.9389\n",
            "Epoch 73/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9243 - val_loss: 0.2200 - val_accuracy: 0.9375\n",
            "Epoch 74/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9246 - val_loss: 0.2201 - val_accuracy: 0.9389\n",
            "Epoch 75/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9253 - val_loss: 0.2203 - val_accuracy: 0.9389\n",
            "Epoch 76/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9249 - val_loss: 0.2204 - val_accuracy: 0.9389\n",
            "Epoch 77/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9256 - val_loss: 0.2204 - val_accuracy: 0.9389\n",
            "Epoch 78/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9256 - val_loss: 0.2207 - val_accuracy: 0.9389\n",
            "Epoch 79/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2124 - accuracy: 0.9253 - val_loss: 0.2207 - val_accuracy: 0.9375\n",
            "Epoch 80/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9256 - val_loss: 0.2208 - val_accuracy: 0.9389\n",
            "Epoch 81/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9263 - val_loss: 0.2207 - val_accuracy: 0.9416\n",
            "Epoch 82/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2118 - accuracy: 0.9256 - val_loss: 0.2209 - val_accuracy: 0.9416\n",
            "Epoch 83/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2117 - accuracy: 0.9253 - val_loss: 0.2211 - val_accuracy: 0.9402\n",
            "Epoch 84/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2116 - accuracy: 0.9263 - val_loss: 0.2211 - val_accuracy: 0.9416\n",
            "Epoch 85/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2114 - accuracy: 0.9256 - val_loss: 0.2213 - val_accuracy: 0.9389\n",
            "Epoch 86/200\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2113 - accuracy: 0.9263 - val_loss: 0.2213 - val_accuracy: 0.9402\n",
            "Epoch 87/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9260 - val_loss: 0.2215 - val_accuracy: 0.9429\n",
            "Epoch 88/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2109 - accuracy: 0.9246 - val_loss: 0.2215 - val_accuracy: 0.9429\n",
            "Epoch 89/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9253 - val_loss: 0.2215 - val_accuracy: 0.9429\n",
            "Epoch 90/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9256 - val_loss: 0.2217 - val_accuracy: 0.9429\n",
            "Epoch 91/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2105 - accuracy: 0.9249 - val_loss: 0.2218 - val_accuracy: 0.9429\n",
            "Epoch 92/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9253 - val_loss: 0.2219 - val_accuracy: 0.9429\n",
            "Epoch 93/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2102 - accuracy: 0.9263 - val_loss: 0.2219 - val_accuracy: 0.9429\n",
            "Epoch 94/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2100 - accuracy: 0.9263 - val_loss: 0.2221 - val_accuracy: 0.9429\n",
            "Epoch 95/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2100 - accuracy: 0.9266 - val_loss: 0.2218 - val_accuracy: 0.9429\n",
            "Epoch 96/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9243 - val_loss: 0.2223 - val_accuracy: 0.9429\n",
            "Epoch 97/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 0.9256 - val_loss: 0.2223 - val_accuracy: 0.9429\n",
            "Epoch 98/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9249 - val_loss: 0.2220 - val_accuracy: 0.9429\n",
            "Epoch 99/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2094 - accuracy: 0.9266 - val_loss: 0.2223 - val_accuracy: 0.9429\n",
            "Epoch 100/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9260 - val_loss: 0.2221 - val_accuracy: 0.9429\n",
            "Epoch 101/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.9249 - val_loss: 0.2224 - val_accuracy: 0.9429\n",
            "Epoch 102/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9256 - val_loss: 0.2225 - val_accuracy: 0.9429\n",
            "Epoch 103/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9256 - val_loss: 0.2226 - val_accuracy: 0.9429\n",
            "Epoch 104/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2090 - accuracy: 0.9266 - val_loss: 0.2226 - val_accuracy: 0.9429\n",
            "Epoch 105/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9249 - val_loss: 0.2226 - val_accuracy: 0.9429\n",
            "Epoch 106/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9260 - val_loss: 0.2228 - val_accuracy: 0.9429\n",
            "Epoch 107/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9260 - val_loss: 0.2229 - val_accuracy: 0.9429\n",
            "Epoch 108/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9260 - val_loss: 0.2229 - val_accuracy: 0.9429\n",
            "Epoch 109/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9249 - val_loss: 0.2231 - val_accuracy: 0.9429\n",
            "Epoch 110/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9256 - val_loss: 0.2229 - val_accuracy: 0.9429\n",
            "Epoch 111/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.9263 - val_loss: 0.2231 - val_accuracy: 0.9416\n",
            "Epoch 112/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9266 - val_loss: 0.2229 - val_accuracy: 0.9429\n",
            "Epoch 113/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9270 - val_loss: 0.2229 - val_accuracy: 0.9416\n",
            "Epoch 114/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9249 - val_loss: 0.2230 - val_accuracy: 0.9416\n",
            "Epoch 115/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2079 - accuracy: 0.9263 - val_loss: 0.2230 - val_accuracy: 0.9429\n",
            "Epoch 116/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9273 - val_loss: 0.2233 - val_accuracy: 0.9416\n",
            "Epoch 117/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2076 - accuracy: 0.9273 - val_loss: 0.2232 - val_accuracy: 0.9416\n",
            "Epoch 118/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.9276 - val_loss: 0.2231 - val_accuracy: 0.9416\n",
            "Epoch 119/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.9276 - val_loss: 0.2232 - val_accuracy: 0.9416\n",
            "Epoch 120/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9263 - val_loss: 0.2234 - val_accuracy: 0.9416\n",
            "Epoch 121/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9276 - val_loss: 0.2235 - val_accuracy: 0.9416\n",
            "Epoch 122/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2072 - accuracy: 0.9273 - val_loss: 0.2233 - val_accuracy: 0.9416\n",
            "Epoch 123/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9270 - val_loss: 0.2235 - val_accuracy: 0.9416\n",
            "Epoch 124/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2070 - accuracy: 0.9270 - val_loss: 0.2233 - val_accuracy: 0.9416\n",
            "Epoch 125/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9276 - val_loss: 0.2235 - val_accuracy: 0.9416\n",
            "Epoch 126/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9263 - val_loss: 0.2237 - val_accuracy: 0.9416\n",
            "Epoch 127/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2067 - accuracy: 0.9276 - val_loss: 0.2234 - val_accuracy: 0.9416\n",
            "Epoch 128/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2067 - accuracy: 0.9276 - val_loss: 0.2235 - val_accuracy: 0.9416\n",
            "Epoch 129/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9276 - val_loss: 0.2235 - val_accuracy: 0.9416\n",
            "Epoch 130/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9280 - val_loss: 0.2235 - val_accuracy: 0.9416\n",
            "Epoch 131/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.9276 - val_loss: 0.2235 - val_accuracy: 0.9416\n",
            "Epoch 132/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.9273 - val_loss: 0.2237 - val_accuracy: 0.9416\n",
            "Epoch 133/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9276 - val_loss: 0.2236 - val_accuracy: 0.9416\n",
            "Epoch 134/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9276 - val_loss: 0.2238 - val_accuracy: 0.9416\n",
            "Epoch 135/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9276 - val_loss: 0.2237 - val_accuracy: 0.9416\n",
            "Epoch 136/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9283 - val_loss: 0.2238 - val_accuracy: 0.9416\n",
            "Epoch 137/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9276 - val_loss: 0.2238 - val_accuracy: 0.9416\n",
            "Epoch 138/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9276 - val_loss: 0.2237 - val_accuracy: 0.9416\n",
            "Epoch 139/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9276 - val_loss: 0.2238 - val_accuracy: 0.9429\n",
            "Epoch 140/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2060 - accuracy: 0.9276 - val_loss: 0.2236 - val_accuracy: 0.9429\n",
            "Epoch 141/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9276 - val_loss: 0.2239 - val_accuracy: 0.9429\n",
            "Epoch 142/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9276 - val_loss: 0.2239 - val_accuracy: 0.9429\n",
            "Epoch 143/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9280 - val_loss: 0.2240 - val_accuracy: 0.9416\n",
            "Epoch 144/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9283 - val_loss: 0.2240 - val_accuracy: 0.9429\n",
            "Epoch 145/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9276 - val_loss: 0.2238 - val_accuracy: 0.9429\n",
            "Epoch 146/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9280 - val_loss: 0.2238 - val_accuracy: 0.9416\n",
            "Epoch 147/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9280 - val_loss: 0.2238 - val_accuracy: 0.9429\n",
            "Epoch 148/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9280 - val_loss: 0.2237 - val_accuracy: 0.9429\n",
            "Epoch 149/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9280 - val_loss: 0.2239 - val_accuracy: 0.9429\n",
            "Epoch 150/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9276 - val_loss: 0.2239 - val_accuracy: 0.9416\n",
            "Epoch 151/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9280 - val_loss: 0.2239 - val_accuracy: 0.9429\n",
            "Epoch 152/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9283 - val_loss: 0.2238 - val_accuracy: 0.9416\n",
            "Epoch 153/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9280 - val_loss: 0.2240 - val_accuracy: 0.9416\n",
            "Epoch 154/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9276 - val_loss: 0.2240 - val_accuracy: 0.9416\n",
            "Epoch 155/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9280 - val_loss: 0.2240 - val_accuracy: 0.9416\n",
            "Epoch 156/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9283 - val_loss: 0.2240 - val_accuracy: 0.9416\n",
            "Epoch 157/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9283 - val_loss: 0.2238 - val_accuracy: 0.9416\n",
            "Epoch 158/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9280 - val_loss: 0.2239 - val_accuracy: 0.9416\n",
            "Epoch 159/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9283 - val_loss: 0.2240 - val_accuracy: 0.9416\n",
            "Epoch 160/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9283 - val_loss: 0.2242 - val_accuracy: 0.9416\n",
            "Epoch 161/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9283 - val_loss: 0.2244 - val_accuracy: 0.9416\n",
            "Epoch 162/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2048 - accuracy: 0.9280 - val_loss: 0.2242 - val_accuracy: 0.9416\n",
            "Epoch 163/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9283 - val_loss: 0.2242 - val_accuracy: 0.9416\n",
            "Epoch 164/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9287 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 165/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9283 - val_loss: 0.2242 - val_accuracy: 0.9416\n",
            "Epoch 166/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2045 - accuracy: 0.9280 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 167/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2045 - accuracy: 0.9280 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 168/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2045 - accuracy: 0.9287 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 169/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2045 - accuracy: 0.9280 - val_loss: 0.2243 - val_accuracy: 0.9416\n",
            "Epoch 170/200\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2044 - accuracy: 0.9276 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 171/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9276 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 172/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9280 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 173/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9283 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 174/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9283 - val_loss: 0.2241 - val_accuracy: 0.9416\n",
            "Epoch 175/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9287 - val_loss: 0.2242 - val_accuracy: 0.9416\n",
            "Epoch 176/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9276 - val_loss: 0.2243 - val_accuracy: 0.9416\n",
            "Epoch 177/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9283 - val_loss: 0.2243 - val_accuracy: 0.9416\n",
            "Epoch 178/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9283 - val_loss: 0.2248 - val_accuracy: 0.9416\n",
            "Epoch 179/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9280 - val_loss: 0.2244 - val_accuracy: 0.9416\n",
            "Epoch 180/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9276 - val_loss: 0.2246 - val_accuracy: 0.9416\n",
            "Epoch 181/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9283 - val_loss: 0.2244 - val_accuracy: 0.9416\n",
            "Epoch 182/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9280 - val_loss: 0.2244 - val_accuracy: 0.9416\n",
            "Epoch 183/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9276 - val_loss: 0.2244 - val_accuracy: 0.9416\n",
            "Epoch 184/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9287 - val_loss: 0.2243 - val_accuracy: 0.9416\n",
            "Epoch 185/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.9280 - val_loss: 0.2245 - val_accuracy: 0.9416\n",
            "Epoch 186/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.9280 - val_loss: 0.2245 - val_accuracy: 0.9416\n",
            "Epoch 187/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9283 - val_loss: 0.2245 - val_accuracy: 0.9416\n",
            "Epoch 188/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9287 - val_loss: 0.2245 - val_accuracy: 0.9416\n",
            "Epoch 189/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9283 - val_loss: 0.2245 - val_accuracy: 0.9416\n",
            "Epoch 190/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9280 - val_loss: 0.2246 - val_accuracy: 0.9416\n",
            "Epoch 191/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9276 - val_loss: 0.2246 - val_accuracy: 0.9416\n",
            "Epoch 192/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9273 - val_loss: 0.2246 - val_accuracy: 0.9416\n",
            "Epoch 193/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9283 - val_loss: 0.2246 - val_accuracy: 0.9416\n",
            "Epoch 194/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9280 - val_loss: 0.2244 - val_accuracy: 0.9416\n",
            "Epoch 195/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9283 - val_loss: 0.2246 - val_accuracy: 0.9416\n",
            "Epoch 196/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9276 - val_loss: 0.2246 - val_accuracy: 0.9416\n",
            "Epoch 197/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9273 - val_loss: 0.2246 - val_accuracy: 0.9416\n",
            "Epoch 198/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2033 - accuracy: 0.9280 - val_loss: 0.2245 - val_accuracy: 0.9416\n",
            "Epoch 199/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9276 - val_loss: 0.2247 - val_accuracy: 0.9416\n",
            "Epoch 200/200\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.2033 - accuracy: 0.9273 - val_loss: 0.2247 - val_accuracy: 0.9416\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7982a0d8b100>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-VVy0udAjpD",
        "outputId": "5465574c-4b62-47f7-c8aa-57b08aae1549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2458 - accuracy: 0.9088\n",
            "Test loss: 0.2458132952451706\n",
            "Test accuracy: 0.9087947607040405\n"
          ]
        }
      ]
    }
  ]
}